{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+v/Aj7SFfOh71oRL4V6Ux",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kelanmail-create/colabs/blob/main/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://tiktokenizer.vercel.app/\n",
        "\n",
        "\n",
        "https://ndingwall.github.io/blog/tokenization"
      ],
      "metadata": {
        "id": "w8aLFpPSK7m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece datasets"
      ],
      "metadata": {
        "id": "SmDi2DTEFh2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sentencepiece as spm\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "WORKDIR = \"/content/tokenizer_demo\"\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "print(\"Workdir:\", WORKDIR)"
      ],
      "metadata": {
        "id": "IfaSJL0lfVBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Karpathy's Tiny Shakespeare raw text\n",
        "!curl -L -o /content/tokenizer_demo/corpus.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "corpus_path = f\"{WORKDIR}/corpus.txt\"\n",
        "!wc -c {corpus_path}\n",
        "!head -n 3 {corpus_path}"
      ],
      "metadata": {
        "id": "apa9378kFZmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(\n",
        "    input_file,\n",
        "    model_prefix,\n",
        "    vocab_size=2000,\n",
        "    model_type=\"unigram\",    # can be \"bpe\" or \"unigram\"\n",
        "    character_coverage=1.0,  # 1.0 for pure English\n",
        "):\n",
        "    cmd = (\n",
        "        f\"--input={input_file} \"\n",
        "        f\"--model_prefix={model_prefix} \"\n",
        "        f\"--vocab_size={vocab_size} \"\n",
        "        f\"--model_type={model_type} \"\n",
        "        f\"--character_coverage={character_coverage} \"\n",
        "        f\"--unk_id=0 --pad_id=1 --bos_id=2 --eos_id=3\"\n",
        "    )\n",
        "    print(\"Training SentencePiece model...\")\n",
        "    spm.SentencePieceTrainer.Train(cmd)\n",
        "    print(\"Saved:\", f\"{model_prefix}.model\")\n",
        "\n",
        "train_tokenizer(\n",
        "    input_file=corpus_path,\n",
        "    model_prefix=f\"{WORKDIR}/spm_unigram\",\n",
        "    model_type=\"unigram\",\n",
        ")\n",
        "train_tokenizer(\n",
        "    input_file=corpus_path,\n",
        "    model_prefix=f\"{WORKDIR}/spm_bpe\",\n",
        "    model_type=\"bpe\",\n",
        ")"
      ],
      "metadata": {
        "id": "SCKAKgIbFagH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_uni = spm.SentencePieceProcessor(model_file=f\"{WORKDIR}/spm_unigram.model\")\n",
        "sp_bpe = spm.SentencePieceProcessor(model_file=f\"{WORKDIR}/spm_bpe.model\")\n",
        "\n",
        "samples = [\n",
        "    \"Tokenization affects efficiency and quality.\",\n",
        "    \"Unbreakable transformations are fascinating!\",\n",
        "    \"Let's analyze subword units produced by BPE and Unigram models.\"\n",
        "]\n",
        "\n",
        "def preview(sp, name):\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    total = 0\n",
        "    for s in samples:\n",
        "        ids = sp.encode(s, out_type=int)\n",
        "        pieces = sp.encode(s, out_type=str)\n",
        "        total += len(ids)\n",
        "        print(f\"â€¢ {s}\\n  {list(zip(pieces, ids))}\\n  ({len(ids)} tokens)\\n\")\n",
        "    print(f\"Avg length: {total/len(samples):.2f} tokens\\n\")\n",
        "\n",
        "preview(sp_uni, \"Unigram\")\n",
        "preview(sp_bpe, \"BPE\")"
      ],
      "metadata": {
        "id": "KeYM8pIBHulR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}