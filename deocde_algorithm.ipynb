{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOokxbcYMYV0KD3xEwSRF+X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kelanmail-create/colabs/blob/main/deocde_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WcTwTdKWh2Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Load GPT Model ----------\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "7vzMlTE9hqlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_ID = model.config.eos_token_id\n",
        "if EOS_ID is None:\n",
        "    EOS_ID = tokenizer.eos_token_id  # GPT-2 50256\n",
        "\n",
        "stop_at_eos_cb = widgets.Checkbox(value=True, description=\"Stop when EOS is sampled\")\n",
        "show_special_in_labels_cb = widgets.Checkbox(value=True, description=\"Show EOS in plot labels\")"
      ],
      "metadata": {
        "id": "dBSveev7gpGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temapurture\n",
        "$$\n",
        "P'(t_i) = \\frac{P(t_i)^{1/T}}{\\sum_{j} P(t_j)^{1/T}}\n",
        "$$\n",
        "\n",
        "Top - k\n",
        "\n",
        "$$\n",
        "V_k = \\text{TopK}\\big(P(t_i \\mid \\text{context}),\\, k\\big)\n",
        "$$\n",
        "\n",
        "$$\n",
        "t_{\\text{next}} \\sim P(t_i \\mid t_i \\in V_k)\n",
        "$$\n",
        "\n",
        "Top - p\n",
        "\n",
        "$$\n",
        "V_p = \\{\\, t_i \\mid \\sum_{j=1}^{i} P(t_j) \\ge p \\,\\}\n",
        "$$\n",
        "\n",
        "$$\n",
        "t_{\\text{next}} \\sim P(t_i \\mid t_i \\in V_p)\n",
        "$$"
      ],
      "metadata": {
        "id": "iTSDeMvxMwYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def top_k_filtering(logits, top_k=0):\n",
        "    if top_k > 0:\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        min_keep = v[..., -1, None]\n",
        "        logits = torch.where(logits < min_keep, torch.tensor(float('-inf'), device=logits.device), logits)\n",
        "    return logits\n",
        "\n",
        "@torch.no_grad()\n",
        "def top_p_filtering(logits, top_p=1.0):\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
        "        probs = F.softmax(sorted_logits, dim=-1)\n",
        "        cumprobs = torch.cumsum(probs, dim=-1)\n",
        "        mask = cumprobs > top_p\n",
        "        mask[..., 0] = False\n",
        "        sorted_logits = sorted_logits.masked_fill(mask, float('-inf'))\n",
        "        scatter = torch.full_like(logits, float('-inf'))\n",
        "        logits = scatter.scatter(-1, sorted_idx, sorted_logits)\n",
        "    return logits\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_from_logits(logits):\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(probs, num_samples=1), probs\n",
        "\n",
        "def greedy_strategy(logits, **kwargs):\n",
        "    return torch.argmax(logits, dim=-1, keepdim=True), F.softmax(logits, dim=-1)\n",
        "\n",
        "def temperature_strategy(logits, temperature=1.0, **kwargs):\n",
        "    logits = logits / max(temperature, 1e-8)\n",
        "    return sample_from_logits(logits)\n",
        "\n",
        "def topk_strategy(logits, top_k=40, temperature=1.0, **kwargs):\n",
        "    logits = top_k_filtering(logits, top_k)\n",
        "    logits = logits / max(temperature, 1e-8)\n",
        "    return sample_from_logits(logits)\n",
        "\n",
        "def topp_strategy(logits, top_p=0.9, temperature=1.0, **kwargs):\n",
        "    logits = top_p_filtering(logits, top_p)\n",
        "    logits = logits / max(temperature, 1e-8)\n",
        "    return sample_from_logits(logits)\n",
        "\n",
        "STRATEGY_MAP = {\n",
        "    \"greedy\": greedy_strategy,\n",
        "    \"temperature\": temperature_strategy,\n",
        "    \"topk\": topk_strategy,\n",
        "    \"topp\": topp_strategy,\n",
        "}\n",
        "\n",
        "\n",
        "class StepDecoder:\n",
        "    def __init__(self, model, tokenizer, prompt, device=\"cpu\"):\n",
        "        self.model = model.eval().to(device)\n",
        "        self.tok = tokenizer\n",
        "        self.device = device\n",
        "        self.input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "        with torch.no_grad():\n",
        "            out = self.model(self.input_ids, use_cache=True)\n",
        "        self.past = out.past_key_values\n",
        "        self.generated = self.input_ids.clone()\n",
        "        self.finished = False\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, strategy=\"greedy\", strategy_kwargs=None):\n",
        "        if self.finished:\n",
        "            return None\n",
        "        strategy_kwargs = strategy_kwargs or {}\n",
        "        last_token = self.generated[:, -1:]\n",
        "        out = self.model(input_ids=last_token, use_cache=True, past_key_values=self.past)\n",
        "        logits = out.logits[:, -1, :]                        # [1, vocab]\n",
        "        self.past = out.past_key_values\n",
        "\n",
        "        fn = STRATEGY_MAP[strategy]\n",
        "        next_id, probs = fn(logits, **strategy_kwargs)       # next_id: [1,1], probs: [1, vocab]\n",
        "\n",
        "        self.generated = torch.cat([self.generated, next_id], dim=-1)\n",
        "\n",
        "        next_id_int = int(next_id.item())\n",
        "        next_prob = float(probs[0, next_id_int].item())\n",
        "\n",
        "        if EOS_ID is not None and next_id_int == EOS_ID:\n",
        "          next_tok = \"<eos>\"\n",
        "          if stop_at_eos_cb.value:\n",
        "              self.finished = True\n",
        "        else:\n",
        "            next_tok = self.tok.decode([next_id_int], skip_special_tokens=True)\n",
        "\n",
        "        return {\n",
        "            \"probs\": probs[0].detach().cpu(),      # [vocab]\n",
        "            \"next_id\": next_id_int,\n",
        "            \"next_prob\": next_prob,\n",
        "            \"next_token_str\": next_tok,\n",
        "            \"text_so_far\": self.tok.decode(self.generated[0], skip_special_tokens=True),\n",
        "        }\n",
        "prompt_box = widgets.Text(\n",
        "    value=\"Q: What is the capital of France?\\n A:\",\n",
        "    description=\"Prompt:\",\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "strategy_dd = widgets.Dropdown(\n",
        "    options=[\"greedy\", \"temperature\", \"topk\", \"topp\"],\n",
        "    value=\"topp\",\n",
        "    description=\"Strategy:\"\n",
        ")\n",
        "\n",
        "temperature_sl = widgets.FloatSlider(\n",
        "    value=0.8, min=0.1, max=2.0, step=0.05, description=\"Temp\"\n",
        ")\n",
        "topk_sl = widgets.IntSlider(\n",
        "    value=40, min=1, max=200, step=1, description=\"Top-K\"\n",
        ")\n",
        "topp_sl = widgets.FloatSlider(\n",
        "    value=0.9, min=0.1, max=1.0, step=0.01, description=\"Top-P\"\n",
        ")\n",
        "topN_sl = widgets.IntSlider(\n",
        "    value=20, min=5, max=100, step=1, description=\"Plot Top-N\"\n",
        ")\n",
        "\n",
        "init_btn = widgets.Button(description=\"Initialize / Reset\", button_style=\"\")\n",
        "step_btn = widgets.Button(description=\"Step ▷ (1 token)\", button_style=\"\")\n",
        "\n",
        "out_plot = widgets.Output()\n",
        "out_text = widgets.Output()\n",
        "\n",
        "def _toggle_param_visibility(*args):\n",
        "    temperature_sl.layout.display = \"none\"\n",
        "    topk_sl.layout.display = \"none\"\n",
        "    topp_sl.layout.display = \"none\"\n",
        "    if strategy_dd.value == \"temperature\":\n",
        "        temperature_sl.layout.display = \"\"\n",
        "    elif strategy_dd.value == \"topk\":\n",
        "        temperature_sl.layout.display = \"\"\n",
        "        topk_sl.layout.display = \"\"\n",
        "    elif strategy_dd.value == \"topp\":\n",
        "        temperature_sl.layout.display = \"\"\n",
        "        topp_sl.layout.display = \"\"\n",
        "\n",
        "strategy_dd.observe(_toggle_param_visibility, names=\"value\")\n",
        "_toggle_param_visibility()\n",
        "\n",
        "decoder_state = {\"decoder\": None}\n",
        "\n",
        "def build_kwargs():\n",
        "    s = strategy_dd.value\n",
        "    if s == \"greedy\":\n",
        "        return {}\n",
        "    if s == \"temperature\":\n",
        "        return {\"temperature\": float(temperature_sl.value)}\n",
        "    if s == \"topk\":\n",
        "        return {\"temperature\": float(temperature_sl.value), \"top_k\": int(topk_sl.value)}\n",
        "    if s == \"topp\":\n",
        "        return {\"temperature\": float(temperature_sl.value), \"top_p\": float(topp_sl.value)}\n",
        "    return {}\n",
        "\n",
        "def on_init_clicked(_):\n",
        "    decoder_state[\"decoder\"] = StepDecoder(model.to(device), tokenizer, prompt_box.value, device=device)\n",
        "    with out_text:\n",
        "        clear_output()\n",
        "        print(\"Decoder is ready. Click 'Step ▷' to generate one token at a time.\")\n",
        "        print(f\"Prompt: {prompt_box.value!r}\")\n",
        "    with out_plot:\n",
        "        clear_output()\n",
        "        # 初始空图\n",
        "        fig = plt.figure(figsize=(8, 4))\n",
        "        plt.title(\"Token probability distribution (click 'Step ▷' to start)\")\n",
        "        plt.xlabel(\"token\")\n",
        "        plt.ylabel(\"probability\")\n",
        "        plt.xticks(rotation=60)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def on_step_clicked(_):\n",
        "    dec = decoder_state.get(\"decoder\", None)\n",
        "    if dec is None:\n",
        "        on_init_clicked(None)\n",
        "        dec = decoder_state[\"decoder\"]\n",
        "\n",
        "    result = dec.step(strategy=strategy_dd.value, strategy_kwargs=build_kwargs())\n",
        "    if result is None:\n",
        "        return\n",
        "\n",
        "    probs = result[\"probs\"]        # [vocab] on CPU\n",
        "    next_id = result[\"next_id\"]\n",
        "    next_prob = result[\"next_prob\"]\n",
        "    next_tok = result[\"next_token_str\"]\n",
        "    text_so_far = result[\"text_so_far\"]\n",
        "\n",
        "    topN = int(topN_sl.value)\n",
        "    top_vals, top_idx = torch.topk(probs, k=topN)\n",
        "    top_vals = top_vals.tolist()\n",
        "    top_idx = top_idx.tolist()\n",
        "    top_tokens = [tokenizer.decode([i], skip_special_tokens=True) or f\"<{i}>\" for i in top_idx]\n",
        "\n",
        "    with out_plot:\n",
        "        clear_output(wait=True)\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.bar(range(len(top_vals)), top_vals)\n",
        "        plt.xticks(range(len(top_tokens)), top_tokens, rotation=60)\n",
        "        plt.xlabel(\"token\")\n",
        "        plt.ylabel(\"probability\")\n",
        "        plt.title(f\"Top-{topN} probs | sampled: id={next_id}, token={repr(next_tok)}, p={next_prob:.4f}\")\n",
        "        if next_id in top_idx:\n",
        "            j = top_idx.index(next_id)\n",
        "            plt.annotate(f\"◀ sampled ({next_prob:.3f})\", xy=(j, top_vals[j]), xytext=(j, max(top_vals)*1.05),\n",
        "                         ha=\"center\", arrowprops=dict(arrowstyle=\"-\"))\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    with out_text:\n",
        "        print(f\"[{strategy_dd.value}] sampled: {repr(next_tok)} (id={next_id}, p={next_prob:.4f})\")\n",
        "        print(\"Text so far:\")\n",
        "        print(text_so_far)\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "init_btn.on_click(on_init_clicked)\n",
        "step_btn.on_click(on_step_clicked)\n",
        "\n",
        "controls_row1 = widgets.HBox([prompt_box])\n",
        "controls_row2 = widgets.HBox([strategy_dd, temperature_sl, topk_sl, topp_sl, topN_sl, stop_at_eos_cb, show_special_in_labels_cb])\n",
        "controls_row3 = widgets.HBox([init_btn, step_btn])\n",
        "\n",
        "display(controls_row1, controls_row2, controls_row3, out_plot, out_text)\n",
        "\n",
        "on_init_clicked(None)"
      ],
      "metadata": {
        "id": "7HiD_7T4bUYh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}